{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChGtEIG48nzM"
      },
      "source": [
        "# **RL Homework Tutorial 1**\n",
        "This notebook contains the homework exercises from tutorial 1 of Deep Reinforcement Learning in AML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py3KVCfC8zh6"
      },
      "source": [
        "# Homework Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzHn3x7u-dUF"
      },
      "source": [
        "## What you'll do in this notebook:\n",
        "\n",
        "During the tutorial, you implemented the core building blocks of REINFORCE:\n",
        "- `discounted_return` - Computing rewards-to-go\n",
        "- `compute_loss` - The REINFORCE loss function  \n",
        "- `get_policy` - Getting stochastic policies from a neural network\n",
        "\n",
        "You also already got a quick look at the whole implementation of the algorithm.\n",
        "\n",
        "Now you'll complete the algorithm by yourself and experiment with it!\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 1: Implement `get_action`\n",
        "Sample actions from your policy for interacting with the environment.\n",
        "\n",
        "### Exercise 2: Implement `train_one_epoch`  \n",
        "Build the main training loop that collects experience and updates the policy.\n",
        "\n",
        "### Exercise 3: Complete the Training Loop\n",
        "Fill in the TODOs to set up and run the full training process.\n",
        "\n",
        "### Exercise 4: Experimentation\n",
        "Try your algorithm on different environments (CartPole, LunarLander, BipedalWalker)\n",
        "and experiment with hyperparameters to see how they affect learning!\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This homework includes some very basic test functions similar to the ones you encountered in the in-class exercises. You can also compare your solutions with the mastersolution of the complete REINFORCE algorithm (available on the polybox).\n",
        "\n",
        "The goal for this homework exercises is to deepen your understanding through hands-on implementation and experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8KF8KNz-lad"
      },
      "source": [
        "# Setup - Run these cells first\n",
        "This section includes the needed imports and the functions already implemented during the in-class exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rrswUBWbwgYC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: swig in /home/lamberto/.local/lib/python3.10/site-packages (4.4.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gymnasium[box2d] in /home/lamberto/.local/lib/python3.10/site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/lamberto/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/lamberto/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/lamberto/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/lamberto/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /home/lamberto/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /home/lamberto/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /home/lamberto/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vSTbNBUqvggp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from typing import Sequence\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e9lecbV0xuQY"
      },
      "outputs": [],
      "source": [
        "def mlp(sizes: Sequence[int], activation=nn.ReLU, output_activation=nn.Identity) -> nn.Sequential:\n",
        "  \"\"\"\n",
        "  Create a simple feedforward neural network.\n",
        "  \"\"\"\n",
        "  layers = []\n",
        "  for j in range(len(sizes)-1):\n",
        "    act = activation if j < len(sizes)-2 else output_activation\n",
        "    layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "  return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7znaERjC2lwZ"
      },
      "outputs": [],
      "source": [
        "def get_policy(obs: torch.Tensor) -> Normal:\n",
        "  \"\"\"\n",
        "  Get the stochastic policy for a given observation (-batch).\n",
        "  Returns a distribution for every action.\n",
        "  \"\"\"\n",
        "  obs = obs.unsqueeze(0) if obs.dim() == 1 else obs  # for single observations that do not have a batch dimension\n",
        "  logits = policy_net(obs)\n",
        "  mean, logstd = logits[:, :n_acts], logits[:, n_acts:]  # split the output layer into mean and logstd\n",
        "  logstd = torch.clamp(logstd, min=-20, max=2)  # for numerical stability\n",
        "  return Normal(mean, torch.exp(logstd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cAzIijKr2okX"
      },
      "outputs": [],
      "source": [
        "def compute_loss(obs: torch.Tensor, act: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Compute the REINFORCE loss for a given batch.\n",
        "  \"\"\"\n",
        "  dist = get_policy(obs)\n",
        "  logp = dist.log_prob(act).sum(dim=-1)\n",
        "  return -(logp * weights).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4oyDxrHn2q_v"
      },
      "outputs": [],
      "source": [
        "def discounted_return(arr: Sequence[float]) -> list[float]:\n",
        "  \"\"\"\n",
        "  Compute the discounted return for a single episode, given a sequence of rewards.\n",
        "  \"\"\"\n",
        "  ret = [0.0] * len(arr)\n",
        "  ret[-1] = arr[-1]\n",
        "  for i in range(len(arr)-2, -1, -1):\n",
        "    ret[i] = arr[i] + gamma * ret[i+1]\n",
        "  return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43KSRf9ZBGB4"
      },
      "source": [
        "# EXERCISE 1: Implement get_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o_Rm4v-dBM-z"
      },
      "outputs": [],
      "source": [
        "def get_action(obs: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Sample an action from the policy for a given observation.\n",
        "\n",
        "    This function is used during environment interaction to select actions.\n",
        "\n",
        "    Args:\n",
        "        obs: Observation tensor of shape (obs_dim,)\n",
        "\n",
        "    Returns:\n",
        "        action: Numpy array of shape (n_acts,) sampled from the policy\n",
        "\n",
        "    Hint: Use get_policy() to get the distribution, then sample from it.\n",
        "    Hint: Remember to convert from torch.Tensor to numpy array and remove batch dimension.\n",
        "    \"\"\"\n",
        "    distr = get_policy(obs)\n",
        "    # NOTE: Take only n_acts \n",
        "    act = distr.sample().squeeze(0)\n",
        "    act = act.numpy()\n",
        "    return act"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8JUu1kBUau"
      },
      "source": [
        "## Test Functions (Feel free to collapse this cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j4_lpUpwBWwt"
      },
      "outputs": [],
      "source": [
        "def test_get_action(get_action_fn):\n",
        "    \"\"\"\n",
        "    Test the get_action implementation.\n",
        "    \"\"\"\n",
        "    # We need to set up a dummy policy network for testing\n",
        "    global policy_net, n_acts\n",
        "    n_acts = 2\n",
        "    policy_net = mlp([8] + [64, 64] + [2*n_acts])\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Test 1: Output is a numpy array\n",
        "    try:\n",
        "        obs = torch.randn(8)\n",
        "        action = get_action_fn(obs)\n",
        "\n",
        "        if isinstance(action, np.ndarray):\n",
        "            print(\"‚úì Test 1: Output is a numpy array\")\n",
        "            results.append(True)\n",
        "        else:\n",
        "            print(f\"‚úó Test 1: Output should be numpy array\")\n",
        "            results.append(False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Test 1: Error - {type(e).__name__}\")\n",
        "        results.append(False)\n",
        "\n",
        "    # Test 2: Output has correct shape\n",
        "    try:\n",
        "        obs = torch.randn(8)\n",
        "        action = get_action_fn(obs)\n",
        "\n",
        "        if action.shape == (2,):\n",
        "            print(\"‚úì Test 2: Output shape is correct\")\n",
        "            results.append(True)\n",
        "        else:\n",
        "            print(f\"‚úó Test 2: Output shape should be (2,)\")\n",
        "            results.append(False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Test 2: Error - {type(e).__name__}\")\n",
        "        results.append(False)\n",
        "\n",
        "    # Test 3: Actions are different across samples (stochastic)\n",
        "    try:\n",
        "        obs = torch.randn(8)\n",
        "        action1 = get_action_fn(obs)\n",
        "        action2 = get_action_fn(obs)\n",
        "\n",
        "        if not np.allclose(action1, action2):\n",
        "            print(\"‚úì Test 3: Policy is stochastic\")\n",
        "            results.append(True)\n",
        "        else:\n",
        "            print(\"‚úó Test 3: Policy should be stochastic\")\n",
        "            results.append(False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Test 3: Error - {type(e).__name__}\")\n",
        "        results.append(False)\n",
        "\n",
        "    # Test 4: Works with multiple different observations\n",
        "    try:\n",
        "        for _ in range(10):\n",
        "            obs = torch.randn(8)\n",
        "            action = get_action_fn(obs)\n",
        "            if action.shape != (2,):\n",
        "                print(\"‚úó Test 4: Failed for random observations\")\n",
        "                results.append(False)\n",
        "                break\n",
        "        else:\n",
        "            print(\"‚úì Test 4: Works with multiple observations\")\n",
        "            results.append(True)\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Test 4: Error - {type(e).__name__}\")\n",
        "        results.append(False)\n",
        "\n",
        "    return sum(results), len(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHeU7GjNBb6w"
      },
      "source": [
        "## Run Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YKagxIFDBiCY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Test 1: Output is a numpy array\n",
            "‚úì Test 2: Output shape is correct\n",
            "‚úì Test 3: Policy is stochastic\n",
            "‚úì Test 4: Works with multiple observations\n",
            "\n",
            "==================================================\n",
            "Results: 4/4 tests passed\n",
            "üéâ All tests passed! You're ready to move on.\n"
          ]
        }
      ],
      "source": [
        "passed, total = test_get_action(get_action)\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Results: {passed}/{total} tests passed\")\n",
        "if passed == total:\n",
        "  print(\"üéâ All tests passed! You're ready to move on.\")\n",
        "else:\n",
        "  print(\"‚ö†Ô∏è  Some tests failed. Review your implementation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WJdGn8ZBms7"
      },
      "source": [
        "# EXERCISE 2: Implement train_one_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zpd7QZV6BtQ-"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch() -> tuple[list, list]:\n",
        "  \"\"\"\n",
        "  Train the policy for one epoch (one policy update).\n",
        "\n",
        "  This function:\n",
        "  1. Collects a batch of experience by running episodes\n",
        "  2. Computes returns-to-go for each timestep\n",
        "  3. Normalizes the returns (for stable learning)\n",
        "  4. Computes the loss and updates the policy\n",
        "\n",
        "  Returns:\n",
        "    batch_rets: List of episode returns\n",
        "    batch_lens: List of episode lengths\n",
        "\n",
        "  Global variables you'll need:\n",
        "  - env: The gymnasium environment\n",
        "  - batch_size: Number of timesteps to collect\n",
        "  - gamma: Discount factor\n",
        "  - optimizer: The optimizer for updating policy_net\n",
        "\n",
        "  Functions you'll use:\n",
        "  - get_action(obs): Sample action from policy\n",
        "  - discounted_return(rewards, gamma): Compute returns-to-go\n",
        "  - compute_loss(obs, acts, weights): Compute policy gradient loss\n",
        "\n",
        "  Hint: Look at the structure from the tutorial slides/video.\n",
        "  Hint: Don't forget to normalize the weights (returns-to-go) before computing loss.\n",
        "  Hint: Remember the standard optimization steps: zero_grad(), backward(), step()\n",
        "  \"\"\"\n",
        "\n",
        "  # NOTE: Quick note, what we mean with store data at each episode means ALL data,\n",
        "  #       batch_act,batch_obs... everything\n",
        "  batch_act = []\n",
        "  batch_obs = []\n",
        "  batch_rewards = []\n",
        "  batch_lengths = []\n",
        "  batch_weights = []\n",
        "\n",
        "  # NOTE: env will be already created once we are here, so no need to create it we just need .reset()\n",
        "  # env = gym.make(env_name, continuous=True, render_mode='human')\n",
        "  obs,_ = env.reset()\n",
        "  rewards = []\n",
        "  # NOTE: loop until is not done, sample an action from distribution, and act in the environment. After all save data in arr\n",
        "  while True:\n",
        "    act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
        "    new_obs,rew,terminated,truncated,_ = env.step(act)\n",
        "\n",
        "    # NOTE: store data, rewards, observations and actions\n",
        "    rewards.append(rew)\n",
        "    batch_obs.append(obs.copy()) # MODIFIED IN PLACE SO NEED TO COPY IT\n",
        "    batch_act.append(act) # REINIZIALIZED EVERY TIME\n",
        "\n",
        "    # NOTE: actually enter new env\n",
        "    obs = new_obs\n",
        "\n",
        "    # NOTE: If the episode is completed, we need to sum the rewards,calculate the length of episode and weights\n",
        "    if terminated or truncated:\n",
        "\n",
        "      reward = sum(rewards)\n",
        "      batch_rewards.append(reward)\n",
        "\n",
        "      length = len(rewards)\n",
        "      batch_lengths.append(length)\n",
        "\n",
        "      batch_weights += discounted_return(rewards)\n",
        "\n",
        "      # NOTE: setuip new environment and fresh rewards array\n",
        "      obs,_ = env.reset()\n",
        "      rewards = []\n",
        "      if len(batch_obs) > batch_size:\n",
        "        break\n",
        "\n",
        "  # NOTE: now prepare the lists for pythorch, only ACT, OBS and WEIGHTS\n",
        "  batch_act = torch.as_tensor(np.array(batch_act), dtype=torch.float32)\n",
        "  batch_obs = torch.as_tensor(np.array(batch_obs), dtype=torch.float32)\n",
        "  batch_weights = torch.as_tensor(np.array(batch_weights), dtype=torch.float32)\n",
        "  # NOTE: remember to normalize weights\n",
        "  batch_weights = (batch_weights - batch_weights.mean()) / (batch_weights.std() + 1e-8).detach() \n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  policy_loss = compute_loss(batch_obs,batch_act,batch_weights)\n",
        "  policy_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return batch_rewards, batch_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfowhMHRCBbK"
      },
      "source": [
        "## Test Functions (Feel free to collapse this cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P6xAYVWhB-ZU"
      },
      "outputs": [],
      "source": [
        "def test_train_one_epoch(train_one_epoch_fn):\n",
        "    \"\"\"\n",
        "    Test the train_one_epoch implementation.\n",
        "    \"\"\"\n",
        "    # Set up environment and global variables for testing\n",
        "    global env, policy_net, n_acts, batch_size, gamma, optimizer\n",
        "\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    n_acts = env.action_space.n\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "\n",
        "    # For CartPole (discrete actions), we need to modify our setup slightly\n",
        "    # But let's use LunarLander continuous for consistency\n",
        "    env = gym.make(\"LunarLander-v3\", continuous=True)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_acts = env.action_space.shape[0]\n",
        "\n",
        "    policy_net = mlp([obs_dim] + [32, 32] + [2*n_acts])\n",
        "    optimizer = Adam(policy_net.parameters(), lr=3e-4)\n",
        "    batch_size = 1000  # Small batch for testing\n",
        "    gamma = 0.99\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Test 1: Returns two lists\n",
        "    try:\n",
        "        batch_rets, batch_lens = train_one_epoch_fn()\n",
        "\n",
        "        if isinstance(batch_rets, list) and isinstance(batch_lens, list):\n",
        "            print(\"‚úì Test 1: Returns two lists\")\n",
        "            results.append(True)\n",
        "        else:\n",
        "            print(\"‚úó Test 1: Should return two lists\")\n",
        "            results.append(False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Test 1: Error - {type(e).__name__}\")\n",
        "        results.append(False)\n",
        "        return sum(results), 4  # Early return if this fails\n",
        "\n",
        "    # Test 2: Lists have same length\n",
        "    try:\n",
        "        batch_rets, batch_lens = train_one_epoch_fn()\n",
        "\n",
        "        if len(batch_rets) == len(batch_lens):\n",
        "            print(\"‚úì Test 2: Return lists have same length\")\n",
        "            results.append(True)\n",
        "        else:\n",
        "            print(\"‚úó Test 2: batch_rets and batch_lens should have same length\")\n",
        "            results.append(False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Test 2: Error - {type(e).__name__}\")\n",
        "        results.append(False)\n",
        "\n",
        "    # Test 3: Episode returns are reasonable\n",
        "    try:\n",
        "        batch_rets, batch_lens = train_one_epoch_fn()\n",
        "\n",
        "        # For LunarLander, returns should be roughly in range [-500, 500]\n",
        "        if all(isinstance(r, (int, float)) for r in batch_rets):\n",
        "            print(\"‚úì Test 3: Episode returns are numeric\")\n",
        "            results.append(True)\n",
        "        else:\n",
        "            print(\"‚úó Test 3: Episode returns should be numeric\")\n",
        "            results.append(False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Test 3: Error - {type(e).__name__}\")\n",
        "        results.append(False)\n",
        "\n",
        "    # Test 4: Policy parameters change after training\n",
        "    try:\n",
        "        # Get initial parameters\n",
        "        initial_params = [p.clone() for p in policy_net.parameters()]\n",
        "\n",
        "        # Run one epoch\n",
        "        batch_rets, batch_lens = train_one_epoch_fn()\n",
        "\n",
        "        # Check if parameters changed\n",
        "        params_changed = any(\n",
        "            not torch.allclose(p1, p2)\n",
        "            for p1, p2 in zip(initial_params, policy_net.parameters())\n",
        "        )\n",
        "\n",
        "        if params_changed:\n",
        "            print(\"‚úì Test 4: Policy parameters updated\")\n",
        "            results.append(True)\n",
        "        else:\n",
        "            print(\"‚úó Test 4: Policy parameters should change after training\")\n",
        "            results.append(False)\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Test 4: Error - {type(e).__name__}\")\n",
        "        results.append(False)\n",
        "\n",
        "    return sum(results), len(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPmuaDmvCPA4"
      },
      "source": [
        "## Run Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Kn2_wlhdCQ5F"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lamberto/.local/lib/python3.10/site-packages/torch/autograd/graph.py:824: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Test 1: Returns two lists\n",
            "‚úì Test 2: Return lists have same length\n",
            "‚úì Test 3: Episode returns are numeric\n",
            "‚úì Test 4: Policy parameters updated\n",
            "\n",
            "==================================================\n",
            "Results: 4/4 tests passed\n",
            "üéâ All tests passed! You're ready to move on.\n"
          ]
        }
      ],
      "source": [
        "passed, total = test_train_one_epoch(train_one_epoch)\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Results: {passed}/{total} tests passed\")\n",
        "if passed == total:\n",
        "  print(\"üéâ All tests passed! You're ready to move on.\")\n",
        "else:\n",
        "  print(\"‚ö†Ô∏è  Some tests failed. Review your implementation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4DTqfW6ChI7"
      },
      "source": [
        "# EXERCISE 3: Complete the Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgCYorhQCnEe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|‚ñç         | 44/1000 [01:05<23:42,  1.49s/it, avg _ret=-144, avg_len=100]"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Now you'll put everything together to train your REINFORCE agent!\n",
        "\n",
        "Fill in the TODOs below to:\n",
        "1. Set up the environment\n",
        "2. Create the policy network\n",
        "3. Create the optimizer\n",
        "4. Run the training loop\n",
        "5. Plot the results\n",
        "\n",
        "This is the complete pipeline you'll use for experimentation in Exercise 4.\n",
        "\"\"\"\n",
        "\n",
        "# Hyperparameters\n",
        "env_name = \"LunarLander-v3\"\n",
        "hidden_sizes = [64, 64]\n",
        "lr = 3e-4\n",
        "epochs = 1000  # You can increase this for better results\n",
        "batch_size = 5000\n",
        "gamma = 0.99\n",
        "plot = True\n",
        "\n",
        "# TODO 1: Create the environment\n",
        "# Hint: Use gym.make() with continuous=Truebatch_rewards for LunarLander-v3\n",
        "env = gym.make(env_name, continuous=True)\n",
        "\n",
        "# TODO 2: Get observation and action dimensions from the environment\n",
        "# Hint: Use env.observation_batch_rewardsspace.shape[0] and env.action_space.shape[0]\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "n_acts = env.action_space.shape[0]\n",
        "\n",
        "# TODO 3: Create the policy network using mlp()\n",
        "# Hint: Input size is obs_dim, output size is 2*n_acts (for mean and logstd)\n",
        "\n",
        "# NOTE: this is the architecture of the neural network, obs as input, hidden layers and 2*acts as output\n",
        "policy_net = mlp([obs_dim, *hidden_sizes, 2*n_acts])\n",
        "\n",
        "# TODO 4: Create the optimizer\n",
        "# Hint: Use Adam optimizer with the policy network parameters and learning rate\n",
        "optimizer = Adam(policy_net.parameters(), lr=lr)\n",
        "\n",
        "# Initialize tracking\n",
        "returns = []\n",
        "std = []\n",
        "\n",
        "# TODO 5: Create a training loop that runs for 'epochs' iterations\n",
        "# Hint: Use tqdm(range(1, epochs+1)) for a progress bar\n",
        "# For each epoch:\n",
        "#   - Call train_one_epoch() to get batch_rets and batch_lens\n",
        "#   - Compute average return and episode length\n",
        "#   - Append to returns list\n",
        "#   - Append std of batch_rets to stds list\n",
        "#   - Update progress bar with avg_ret and avg_len\n",
        "\n",
        "# YOUR CODE HERE for the training loop\n",
        "progress_bar = tqdm(range(1, epochs+1))\n",
        "for _ in progress_bar:\n",
        "  batch_rewards, batch_lengths = train_one_epoch()\n",
        "  avg_ret, avg_len = np.mean(batch_rewards), np.mean(batch_lengths)\n",
        "  returns.append(avg_ret)\n",
        "\n",
        "  std_ret = np.std(batch_rewards)\n",
        "  std.append(std_ret)\n",
        "\n",
        "  progress_bar.set_postfix({'avg _ret': f'{avg_ret:5.0f}', \"avg_len\": f\"{avg_len:5.0f}\"})\n",
        "\n",
        "\n",
        "if plot:\n",
        "  plt.plot(returns)\n",
        "  plt.fill_between(range(len(returns)), np.array(returns) - np.array(std), np.minimum(300, np.array(returns) + np.array(std)), alpha=0.3)\n",
        "  plt.grid()\n",
        "  goal = {\"LunarLander-v3\": 200, \"BipedalWalker-v3\": 300}.get(env_name, 0)\n",
        "  plt.axhline(goal, color='r', linestyle='--')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('average return')\n",
        "  timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "  plt.savefig(f\"Reinforce_training_{timestamp}.png\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ1hEIKYC6Th"
      },
      "source": [
        "# EXERCISE 4: Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZqZxpBFDKsU"
      },
      "source": [
        "Try running REINFORCE on different environments and with different hyperparameters.\n",
        "\n",
        "To learn more about the specific enviroments, visit [Gymnasium](https://gymnasium.farama.org/).\n",
        "\n",
        "## Suggested Experiments:\n",
        "\n",
        "### 1. Different Environments:\n",
        "\n",
        "**CartPole-v1** (Easy, fast learning ~100 epochs):\n",
        "- Change env_name to \"CartPole-v1\"\n",
        "- Note: CartPole has DISCRETE actions, so you'll need to modify the structure of the policy network and the functions get_policy and get_action\n",
        "  - Remember how the output layer should look like if we want a stochastic policy for discrete actions?\n",
        "- Or skip this and stick with continuous control tasks\n",
        "\n",
        "**LunarLander-v3** (Medium difficulty):\n",
        "- Goal: 200+ average return\n",
        "- Typical training: 500-800 epochs\n",
        "- Good for testing different hyperparameters\n",
        "\n",
        "**BipedalWalker-v3** (Hard, slow learning):\n",
        "- Change env_name to \"BipedalWalker-v3\" (The continuous parameter is not available here)\n",
        "- Goal: 300+ average return\n",
        "- Needs 800-1000+ epochs\n",
        "- Very sensitive to hyperparameters\n",
        "- Great for seeing the effect of batch_size and lr\n",
        "\n",
        "### 2. Hyperparameter Exploration:\n",
        "\n",
        "Try varying:\n",
        "- **Learning rate (lr)**\n",
        "  - Too high: Unstable learning, policy might collapse\n",
        "  - Too low: Very slow learning\n",
        "  \n",
        "- **Batch size**\n",
        "  - Larger: More stable gradients but slower epochs\n",
        "  - Smaller: Faster epochs but noisier gradients\n",
        "  \n",
        "- **Hidden layer sizes**\n",
        "  - Bigger networks: More capacity but slower training\n",
        "  \n",
        "- **Discount factor (gamma)**\n",
        "  - Higher gamma: Agent plans further ahead\n",
        "  - Lower gamma: More short-sighted behavior\n",
        "\n",
        "### 3. Things to Observe:\n",
        "\n",
        "- How does the learning curve change?\n",
        "- Does the agent solve the task?\n",
        "- How stable is training?\n",
        "\n",
        "### Tips:\n",
        "- Start with LunarLander-v3 (it trains in reasonable time)\n",
        "- Make ONE change at a time to see its effect\n",
        "- Keep notes on what works and what doesn't\n",
        "- Compare learning curves side-by-side\n",
        "\n",
        "**To experiment:** Simply modify the hyperparameters in Exercise 3 and re-run!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "n8KF8KNz-lad",
        "43KSRf9ZBGB4",
        "jP8JUu1kBUau",
        "UHeU7GjNBb6w",
        "5WJdGn8ZBms7",
        "NfowhMHRCBbK",
        "UPmuaDmvCPA4",
        "I4DTqfW6ChI7",
        "sQ1hEIKYC6Th"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
